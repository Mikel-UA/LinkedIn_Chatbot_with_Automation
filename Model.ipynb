{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91940538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\n",
    "from transformers import pipeline, Conversation\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "import mtranslate\n",
    "import nltk.tokenize as nt\n",
    "import nltk\n",
    "import config\n",
    "    \n",
    "\n",
    "\n",
    "def initiate_conversational_model():\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"./1-blenderbot-3B\")\n",
    "        model = AutoModel.from_pretrained(\"./1-blenderbot-3B\")\n",
    "    except:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-3B\")\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/blenderbot-3B\")\n",
    "\n",
    "        tokenizer.save_pretrained(\"./1-blenderbot-3B\")\n",
    "        model.save_pretrained(\"./1-blenderbot-3B\")\n",
    "    return tokenizer,model\n",
    "\n",
    "\n",
    "#Initiate question answering model\n",
    "\n",
    "def initiate_answering_model():\n",
    "    \n",
    "    try:\n",
    "        tokenizer1 = AutoTokenizer.from_pretrained(\"./2-roberta-base-squad2\")\n",
    "        model1 = AutoModel.from_pretrained(\"./2-roberta-base-squad2\")\n",
    "    except:\n",
    "        tokenizer1 = AutoTokenizer.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "        model1 = AutoModelForQuestionAnswering.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "\n",
    "        tokenizer1.save_pretrained(\"./2-roberta-base-squad2\")\n",
    "        model1.save_pretrained(\"./2-roberta-base-squad2\")\n",
    "    \n",
    "    return tokenizer1, model1\n",
    "\n",
    "\n",
    "#Initiate pharaphrasing model\n",
    "\n",
    "def initiate_pharaphrasing_model():\n",
    "    \n",
    "    try:\n",
    "        tokenizer2 = AutoTokenizer.from_pretrained(\"./3-pegasus_paraphrase\")\n",
    "        model2 = AutoModel.from_pretrained(\"./3-pegasus_paraphrase\")\n",
    "    except:\n",
    "        tokenizer2 = AutoTokenizer.from_pretrained(\"tuner007/pegasus_paraphrase\", use_fast=False)\n",
    "        model2 = AutoModelForSeq2SeqLM.from_pretrained(\"tuner007/pegasus_paraphrase\")\n",
    "\n",
    "        tokenizer2.save_pretrained(\"./3-pegasus_paraphrase\")\n",
    "        model2.save_pretrained(\"./3-pegasus_paraphrase\")\n",
    "    \n",
    "    return tokenizer2, model2\n",
    "\n",
    "def initiate_translation_es_en_model():\n",
    "    \n",
    "    try:\n",
    "        es_en_tokenizer = AutoTokenizer.from_pretrained(\"./4-helsinki-NLP-es-en\")\n",
    "        es_en_model = AutoModel.from_pretrained(\"./4-helsinki-NLP-es-en\")\n",
    "    except:\n",
    "        es_en_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-es-en\")\n",
    "        es_en_model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-es-en\")\n",
    "\n",
    "        es_en_tokenizer.save_pretrained(\"./4-helsinki-NLP-es-en\")\n",
    "        es_en_model.save_pretrained(\"./4-helsinki-NLP-es-en\")\n",
    "\n",
    "    return es_en_tokenizer,es_en_model\n",
    "\n",
    "def initiate_translation_en_es_model():\n",
    "   \n",
    "    try:\n",
    "        en_es_tokenizer = AutoTokenizer.from_pretrained(\"./5-helsinki-NLP-en-es\")\n",
    "        en_es_model = AutoModel.from_pretrained(\"./5-helsinki-NLP-en-es\")\n",
    "    except:\n",
    "        en_es_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\")\n",
    "        en_es_model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\")\n",
    "\n",
    "        en_es_tokenizer.save_pretrained(\"./5-helsinki-NLP-en-es\")\n",
    "        en_es_model.save_pretrained(\"./5-helsinki-NLP-en-es\")\n",
    "    \n",
    "    return en_es_tokenizer,en_es_model\n",
    "\n",
    "def translate_es_en(text):\n",
    "    encoded_input = es_en_tokenizer(text, return_tensors=\"pt\")\n",
    "    output = es_en_model.generate(**encoded_input)\n",
    "    out_text = es_en_tokenizer.batch_decode(output,skip_special_tokens=True)\n",
    "    \n",
    "    return out_text\n",
    "\n",
    "def translate_es_en_gt(text):\n",
    "    return mtranslate.translate(text, \"es\", \"en\")\n",
    "    \n",
    "def translate_en_es_gt(text):\n",
    "    return mtranslate.translate(text, \"en\", \"es\")\n",
    "\n",
    "\n",
    "def translate_en_es(text):\n",
    "    encoded_input = en_es_tokenizer(text, return_tensors=\"pt\")\n",
    "    output = en_es_model.generate(**encoded_input)\n",
    "    out_text = en_es_tokenizer.batch_decode(output,skip_special_tokens=True)\n",
    "    return out_text\n",
    "\n",
    "def create_conversational_pipeline(model, tokenizer):\n",
    "    conversational_pipeline = pipeline(\"conversational\", model=model, tokenizer=tokenizer)\n",
    "    return conversational_pipeline\n",
    "\n",
    "\n",
    "def create_answering_pipeline(model1, tokenizer1):\n",
    "    question_answering = pipeline(\"question-answering\", model=model1, tokenizer=tokenizer1)\n",
    "    return question_answering\n",
    "\n",
    "\n",
    "response=\"\"\n",
    "def get_contextual(question,question_answering):\n",
    "    result=None\n",
    "    result = question_answering(question=question, context=config.context)\n",
    "\n",
    "    print(\"Answer:\", result['answer'])\n",
    "    print(\"Score:\", result['score'])\n",
    "\n",
    "    return result['answer'], result['score']\n",
    "\n",
    "\n",
    "\n",
    "sentence=\"\"\n",
    "def tokenize_question(question):\n",
    "    ss=nt.sent_tokenize(question)\n",
    "    tokenized_sent=[nt.word_tokenize(sent) for sent in ss]\n",
    "    \n",
    "    pos_sentences=[nltk.pos_tag(sent) for sent in tokenized_sent]\n",
    "    print(pos_sentences)\n",
    "\n",
    "    #list to save verbs and sustantives\n",
    "    useful_tokens = []\n",
    "    for item in pos_sentences[0]:\n",
    "        print(item)\n",
    "        if 'VB' in item[1] or 'NN' in item[1]:\n",
    "            useful_tokens.append(item[0])\n",
    "\n",
    "    #first verb must be ignored, it is a question so if we save it the paraphraser will make a question with it\n",
    "\n",
    "    \n",
    "    if \"?\" in question and bool(useful_tokens):\n",
    "        del useful_tokens[0]\n",
    "    \n",
    "    result=\"\"\n",
    "    for item in useful_tokens:\n",
    "        result=result+str(item+\" \")\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "def get_conversation(question, tokenizer, model):\n",
    "    inputs = tokenizer([question], return_tensors=\"pt\")\n",
    "    reply_ids = model.generate(**inputs)\n",
    "    return tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "\n",
    "def get_text_to_pharaphrase(init, sentence, response):\n",
    "    #Execute pharaphrasing\n",
    "    text_to_pharaphrase=init + \" \" +sentence+\" \"+response+\".\"\n",
    "    Â¡return text_to_pharaphrase\n",
    "\n",
    "text_to_pharaphrase=\"\"\n",
    "\n",
    "def get_pharaphrased(text_to_pharaphrase, tokenizer2, model2):\n",
    "    \n",
    "    batch = tokenizer2(text_to_pharaphrase, truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "    translated = model2.generate(**batch)\n",
    "    result = tokenizer2.batch_decode(translated, skip_special_tokens=True)\n",
    "    return result[0]\n",
    "\n",
    "def generate_response(question, pipeline):\n",
    "\n",
    "    #Introducing context\n",
    "    context = config.context\n",
    "    \n",
    "    # Get only the question in all the text received\n",
    "    start = '.'\n",
    "    end = '?'\n",
    "    s = question\n",
    "    print(s[s.find(start)+len(start):s.rfind(end)]+\"?\")\n",
    "    question_splited=s[s.find(start)+len(start):s.rfind(end)]+\"?\"\n",
    "    \n",
    "    answer=None\n",
    "    score=None\n",
    "    \n",
    "    #if contextual isnt good enough, there is no answer, so the conversational will be used\n",
    "    answer, score = get_contextual(question_splited, question_answering)\n",
    "    if score<0.15:\n",
    "        answer=get_conversational_pipeline(pipeline, question)\n",
    "        answer=answer.generated_responses[-1]\n",
    "    else:\n",
    "        tokens=tokenize_question(question)\n",
    "        if \"your\" in question:\n",
    "            init=\"My \" \n",
    "        else:\n",
    "            init=\"I \"\n",
    "\n",
    "        text_to_pharaphrase=get_text_to_pharaphrase(init, tokens, answer)\n",
    "        answer=get_pharaphrased(text_to_pharaphrase, tokenizer2, model2)\n",
    "        print(answer)\n",
    "        \n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70ed3057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_conversational_pipeline(pipeline_name):\n",
    "    pipeline_name = Conversation(\".\")\n",
    "    return pipeline_name\n",
    "\n",
    "def get_conversational_pipeline(pipeline_name, text):\n",
    "    pipeline_name.add_user_input(text, overwrite=True)\n",
    "    result=conversational_pipeline(pipeline_name)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd3a314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#Initiate conversational model\n",
    "tokenizer, model = initiate_conversational_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d2cb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_pipeline= create_conversational_pipeline(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73c8d7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate answering model\n",
    "tokenizer1, model1 = initiate_answering_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdfe955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answering= create_answering_pipeline(model1, tokenizer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "565cd7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate pharaphrasing model\n",
    "tokenizer2, model2 = initiate_pharaphrasing_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13137ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating translation models\n",
    "es_en_tokenizer, es_en_model= initiate_translation_es_en_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e50fdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_es_tokenizer, en_es_model= initiate_translation_en_es_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
